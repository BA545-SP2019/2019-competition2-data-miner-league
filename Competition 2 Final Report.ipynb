{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Miner League Competition 2 Final Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Question\n",
    "\n",
    "- Our client is seeking advanced and novel methods to prepare the collected data, to prepare for the classification purpose. Our goal is to to design, implement, and deploy a classification analysis to predict whether a credit card customer will default the payment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applicable Packags\n",
    " - Before reading in the csv file for competition 2 there were several packages to be loaded into the notebook for use. These packages enabled us to use visualization methods as well as feature optimization methods.\n",
    "     - Imported packages consisited of NUMPY, PANDAS, MATLPOTLIB, SEABORN and SKLEARN\n",
    "     - Sub packages used for data models include model_selection, croos_val_score, train_test_split, metrics, confusion_matrix, classification_report, roc_auc_score and accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "- During the initial data exploration process we used the data dictionary to determine the data types of the stored variables and whether or not these values were continuous or binary.\n",
    "    - We identified that the data set contained 24 attributes and 30000 rows. The initial dataset contained no missing data and determined descritive statistics for each variable.\n",
    "- Basic Exploration\n",
    "    - We explored the categorical and numerical data. A few points of interest were education, marriage and credit limit based on age. In terms of education we found that there was no real trend however people with Bachelors degrees were most likely to default out of the sample size. In terms of marrital status, those who were single were more likely to default than those married and there was no differentiation based on defaulting and age. One might expect younger people more likely to default based on less experience with responsibility however this was not the case.\n",
    "- Correlation\n",
    "    - Using Seaborn we determined the relationships between the explanatory and target variables (Will the client default on the payment)\n",
    "        - Variables PAY_1 - PAY_6 had the highes correlation to the Default status and PAY_AMT1 - PAY_AMT6 had the lowest correlation score to the Default status variable. Correlation of Default status is lowest at -.15 with LIMIT_BAL. This negative correlation indicates that the higher the credit limit, lower the chance of default. Correlation of Default Status is highestat .4 with PAY_1. THis positive correlation indicates that the longer the period of delayed Payment, higher chances of defatult occured. The clients payment behavior is a strong indication of their chances of defaulting.\n",
    "    - The correlation analysis was a first step in determing which explanatory variables were the biggest default status drivers.\n",
    "- Target imbalance\n",
    "    - To handle target imbalcance within the variable we imported SMOTE from imblearn.oversampling and resampled the the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection Using RFE\n",
    "- using the Feauture selection module within the sklearn package we re ran the logisitc regression using the variables that were deemed to be the most efficient. The feature selection ranked the variables in order of the standard score. The three most important features were Pay_AMT1, PAY_AMT2 and PAY_AMT4. The model score using these features yielded an accuracy of 77.8%\n",
    "\n",
    "### Feature Importance\n",
    "- wanted to cross check the top three variables found about so we created a feature importance model and the three most important explanatory variables according this method were PAY_2, PAY_1 and DEFAULT_Status which is the target variable.\n",
    "- We re initialized the logistic regression model and used PAY_1, Limit_BAL and Bill_AMT1 asour new explanatory variables and this yielded a new accuracy of 75%\n",
    "- We also re ran our K Neighbors classification model and recieved an accuracy score of 63% which is lower than the initial k Neighbors classifier model.\n",
    "- Using ne new features also re ran the Decision Tree Classifier and yielded an accuracy score of 71.3%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "- After determining what features might be used to establish an optimal model we wanted to create a logistic regression pipeline examine results using every variable and using the variables that were obtained using recursive feature elimination methods.\n",
    "- Using all of the features we ran a regular logistic regresion, cross validation and a hyper parameter optimization method. Respectively, model scores were 76%, .7 and 69%.\n",
    "- We then eliminated features and retested using the same models. Respectively, the models scored 78%. 68 and 78%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-NN Classifier Model\n",
    "- We took the same approach as the logistic regression pipeline. We had two k-NN classifier pipelines, one with all of the variables and one with a narrowed selection.\n",
    "- Using all of the variables we tested three variations, a regular K-NN Classifier, cross validation and hyperparameter optimization. Respectively the model scores were .69, .75 and .65\n",
    "- We then eliminated features and retested using the same models. Respectively, the models scored .72, .62 and .72\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier\n",
    "- Continuing on with the idea of the pipeline using split variables we also used this approach using a decision tree classifier\n",
    "- one pipeline used all of the variables and the other only used the optimal variables. Both models both used a regular decision tree classifier, a cross validation model and a hyperparameter optimization method.\n",
    "- The model using all of the variables respectively recieved scores of .72, .73 and .76\n",
    "- The model using the optimal variables respectively received scores of  .72, .69 and .75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forrest Classifier\n",
    "- Continuing on with the idea of the pipeline using split variables we also used this approach using a decision tree classifier\n",
    "- one pipeline used all of the variables and the other only used the optimal variables. Both models both used a regular decision tree classifier, a cross validation model and a hyperparameter optimization method.\n",
    "- The model using all of the variables respectively recieved scores of .85, .8 and .79\n",
    "- The model using the optimal variables respectively received scores of .66, 70 and .74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application\n",
    "- Being able to identfy who might default on a credit card payment is a very powerful tool. Building models is just the first step. Crossing the bridge from models to applications is a crucial aspect of business analytics. Identying the traits of defaulting customers can be used in a number of ways. One of these ways is by using this information to identify where to set a potential customers credit limit for a given month. This can save the credit card company money and also make them money through inducuing increased interest. The company can target customers who are likely to default and raise their interest rates accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
